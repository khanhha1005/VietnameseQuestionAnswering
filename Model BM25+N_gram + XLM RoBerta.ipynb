{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install required packages"
      ],
      "metadata": {
        "id": "iBY_xk9nu3-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyserini faiss-cpu transformers"
      ],
      "metadata": {
        "id": "gQ3Oinoz_frx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download resources"
      ],
      "metadata": {
        "id": "pqEuJohiux-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Documents"
      ],
      "metadata": {
        "id": "Vj1YIeuDvW3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1UWFMJFq_N9GZgYJr_2ErFg6g8Bo9E2VQ"
      ],
      "metadata": {
        "id": "Bpcuffi6YPhJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea54a885-76af-407d-a1df-915b972c6f95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1UWFMJFq_N9GZgYJr_2ErFg6g8Bo9E2VQ\n",
            "To: /content/wikipedia_20220620_cleaned.jsonl\n",
            "100% 1.57G/1.57G [00:27<00:00, 56.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretrained model weight"
      ],
      "metadata": {
        "id": "g9nDyDwIvYuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1XKcuDTaSWXIXZ_1yqQiowcrcz0QFrKEb\n",
        "!unzip vi-mrc-base.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "541JdZplvbok",
        "outputId": "af4523a8-e219-4708-8f79-fd0cb478b603"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1XKcuDTaSWXIXZ_1yqQiowcrcz0QFrKEb\n",
            "To: /content/vi-mrc-base.zip\n",
            "100% 879M/879M [00:10<00:00, 83.2MB/s]\n",
            "Archive:  vi-mrc-base.zip\n",
            "  inflating: vi-mrc-base/pytorch_model.bin  \n",
            "  inflating: vi-mrc-base/tokenizer_config.json  \n",
            "  inflating: vi-mrc-base/config.json  \n",
            "  inflating: vi-mrc-base/readme.txt  \n",
            "  inflating: vi-mrc-base/special_tokens_map.json  \n",
            "  inflating: vi-mrc-base/tokenizer.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import library"
      ],
      "metadata": {
        "id": "rOAINxi3b1j5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from heapq import heappush, heappop\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline"
      ],
      "metadata": {
        "id": "fYr-7U1bnVgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing function"
      ],
      "metadata": {
        "id": "pZy8jKUPb6H6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "  text = text.replace('BULLET::::-', '')\n",
        "  text = re.sub('<.+?>', '', text)\n",
        "  text = re.sub('==.+==', '', text)\n",
        "  text = re.sub('[,;:\\\\?\\\\(\\\\)\\\\[\\\\]\\\\{\\\\}\\\\<>|\\'\"=\\\\-–—…/\\\\+\\\\!\\\\*－_]', ' ', text)\n",
        "  text = re.sub('\\\\.+', '.', text)\n",
        "  text = re.sub('\\\\.\\s+', '. ', text)\n",
        "  text = re.sub('\\s*\\n', '. ', text)\n",
        "  text = re.sub('\\s+', ' ', text)\n",
        "  # text = text.lower()\n",
        "  return text"
      ],
      "metadata": {
        "id": "J9arP13u_qRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_passages(text):\n",
        "  text = re.sub('BULLET::::[\\\\-\\d]+', '', text)\n",
        "  text = re.sub('<.+?>', '', text)\n",
        "  text = re.sub('==.+==', '', text)\n",
        "  current_text = ''\n",
        "  current_len = 0\n",
        "  passages = []\n",
        "  for p in re.split('\\\\.\\s+', text):\n",
        "    p = re.sub('\\n\\s*', '. ', p)\n",
        "    p = p.strip()\n",
        "    if p == '':\n",
        "      continue\n",
        "    passages.append(p + '.')\n",
        "  return passages"
      ],
      "metadata": {
        "id": "8Vaev2lPxX2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BM25 indexing cache for document ranking"
      ],
      "metadata": {
        "id": "kbl9XWHKcNak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create necessary directories, dump documents as Lucene input file"
      ],
      "metadata": {
        "id": "gS2VnrATcmpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.isdir('index'):\n",
        "  os.mkdir('index')\n",
        "if not os.path.isdir('input'):\n",
        "  os.mkdir('input')\n",
        "def reformat_documents():\n",
        "  documents = []\n",
        "  with open('wikipedia_20220620_cleaned.jsonl', 'r') as f:\n",
        "    for line in f:\n",
        "      data = json.loads(line)\n",
        "      text = preprocess_text(data['text'])\n",
        "      documents.append({'id': data['id'], 'contents': text})\n",
        "  with open('input/documents.json', 'w') as f:\n",
        "    json.dump(documents, f)\n",
        "reformat_documents()"
      ],
      "metadata": {
        "id": "5dBIw-Skvmly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Lucene index file for caching"
      ],
      "metadata": {
        "id": "77w8Fe0Qcx5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pyserini.index.lucene --collection JsonCollection --input input --language vi --index index --generator DefaultLuceneDocumentGenerator --threads 1 --storePositions --storeDocvectors"
      ],
      "metadata": {
        "id": "pNRmBkIXzQmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize searcher"
      ],
      "metadata": {
        "id": "pof1-i0ZdB8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyserini.search.lucene import LuceneSearcher\n",
        "\n",
        "searcher = LuceneSearcher('index')\n",
        "searcher.set_bm25(0.9, 0.4)\n",
        "searcher.set_language('vi')"
      ],
      "metadata": {
        "id": "8pHJ2SggDNZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N-gram processor for passage ranking"
      ],
      "metadata": {
        "id": "Qc-oH9JFdpxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NGram:\n",
        "  def __init__(self, text, reformat=True):\n",
        "    text = preprocess_text(text)\n",
        "    text = text.replace('. ', ' ')\n",
        "    lTokens = []\n",
        "    uTokens = []\n",
        "    for w in text.split():\n",
        "      if any(c.isupper() for c in w):\n",
        "        uTokens.append(w)\n",
        "      else:\n",
        "        lTokens.append(w)\n",
        "    self.uni = self.get_ngrams(lTokens, 1)\n",
        "    self.bi = self.get_ngrams(lTokens, 2)\n",
        "    self.tri = self.get_ngrams(lTokens, 3)\n",
        "\n",
        "    self.Uni = self.get_ngrams(uTokens, 1)\n",
        "    self.Bi = self.get_ngrams(uTokens, 2)\n",
        "    self.Tri = self.get_ngrams(uTokens, 3)\n",
        "\n",
        "  def get_ngrams(self, tokens, n=2):\n",
        "    return set([tuple(tokens[i:i + n]) for i in range(len(tokens) - n + 1)])\n",
        "\n",
        "  def score(self, o):\n",
        "    return len(self.uni & o.uni) + len(self.bi & o.bi) + len(self.tri & o.tri) + 2 * (len(self.Uni & o.Uni) + len(self.Bi & o.Bi) + len(self.Tri & self.Tri))"
      ],
      "metadata": {
        "id": "Wpc6Ki7ETq8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Open wikipedia data"
      ],
      "metadata": {
        "id": "RwPr4WkYcaJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = dict()\n",
        "\n",
        "with open('wikipedia_20220620_cleaned.jsonl', 'r') as f:\n",
        "  for line in f:\n",
        "    data = json.loads(line)\n",
        "    text = data['text']\n",
        "    text = text.replace('BULLET::::-', '')\n",
        "    text = re.sub('<.+?>', '', text)\n",
        "    text = re.sub('==.+==', '', text)\n",
        "    documents[data['id']] = (data['title'], text)"
      ],
      "metadata": {
        "id": "fdWJ4LUh2MmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load QA model"
      ],
      "metadata": {
        "id": "eQRdvGrOdWWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = 'vi-mrc-base'\n",
        "qa_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "qa_model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "qa_pipeline = pipeline('question-answering', model=qa_model, tokenizer=qa_tokenizer, device=device)"
      ],
      "metadata": {
        "id": "JyE3lPmkupWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_question = 'Sau khi qua đời, vua Lý Nhân Tông truyền ngôi cho ai'\n",
        "sample_text = 'Năm 1128, Lý Nhân Tông qua đời, hưởng thọ 63 tuổi, Dương Hoán lúc đó mới 11 tuổi lên nối ngôi, tức là Lý Thần Tông'\n",
        "qa_pipeline({'question': sample_question, 'context': sample_text})"
      ],
      "metadata": {
        "id": "Gs0VpDBR1L8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QA pipeline step-by-step"
      ],
      "metadata": {
        "id": "9AnTIU61hfSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_question = 'Ai hiện là giám đốc điều hành Xiaomi'\n",
        "# sample_question = 'Tổng thống đầu tiên của Mỹ là ai?'\n",
        "# sample_question = 'Trong thần thoại hy lạp vị thần tình yêu có tên là gì?'\n",
        "# sample_question = 'Đạo diễn phim Titanic là ai?'\n",
        "# sample_question = 'Tổng thống Hoa Kỳ thứ 45 là ai'\n",
        "# sample_question = 'Hiện nay ai là tổng bí thư nước Việt Nam'\n",
        "# sample_question = 'Sau khi qua đời, vua Lý Nhân Tông truyền ngôi cho ai'\n",
        "# sample_question = 'Thuyết tương đối đặc biệt và thuyết tương đối tổng quát là ai phát minh ra?'\n",
        "# sample_question = 'Nhà thờ Đức Bà Paris được xây dựng theo lối kiến trúc nào'\n",
        "# sample_question = 'Ngôi chùa đúc hoàn toàn bằng đồng ở Việt Nam'\n",
        "# sample_question = 'Trương Quang Nghĩa sinh ngày bao nhiêu'"
      ],
      "metadata": {
        "id": "HOzvt9Omch5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create ngram for question"
      ],
      "metadata": {
        "id": "ur0ML_AGxaYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_ngram = NGram(sample_question)"
      ],
      "metadata": {
        "id": "jdKvT6WwEh0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rank documents using BM25"
      ],
      "metadata": {
        "id": "Dx9YLy5Hxco_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hits = searcher.search(preprocess_text(sample_question), k=100)\n",
        "document_ranks = []\n",
        "for hit in hits:\n",
        "  document_ranks.append((hit.score, hit.docid))\n",
        "for idx, (score, text) in enumerate(document_ranks):\n",
        "  print(idx, score, documents[text][0])"
      ],
      "metadata": {
        "id": "Ngdad4hr-iqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rank passages in 2 stages.\n",
        "\n",
        "Stage 1: Rank passages in each document.\n",
        "\n",
        "Stage 2: Rank passages across all documents."
      ],
      "metadata": {
        "id": "Ia5-xquHxg3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "passage_ranks = []\n",
        "for idx, (doc_score, doc) in enumerate(document_ranks):\n",
        "  doc_title, doc_content = documents[doc]\n",
        "  ranks = []\n",
        "  for p in to_passages(doc_content):\n",
        "    pas_ngram_score = question_ngram.score(NGram(p)) + question_ngram.score(NGram(doc_title))\n",
        "    ranks.append((pas_ngram_score, p))\n",
        "  ranks.sort(reverse=True)\n",
        "  passage_ranks += ranks[:5]\n",
        "  for r in ranks[:5]:\n",
        "    print(r[0], question_ngram.score(NGram(doc_title)), doc_title)\n",
        "    print(r[1])\n",
        "    print('\\n')\n",
        "passage_ranks.sort(reverse=True)\n",
        "# passage_ranks[:50]"
      ],
      "metadata": {
        "id": "qFKtMBWdCvpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract answer for each passage with confident threshold, skip answer in blacklist dictionary"
      ],
      "metadata": {
        "id": "nP04kTuDx2Da"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answers = {}\n",
        "blacklist = set(['kiêm', 'Quỳnh'])\n",
        "for p_score, p in passage_ranks[:50]:\n",
        "  res = qa_pipeline({'question': sample_question, 'context': p})\n",
        "  score = res['score']\n",
        "  answer = res['answer']\n",
        "  answer = re.sub('[,\\\\.\\\\(\\\\);:\\\"]', '', answer)\n",
        "  if answer != '':\n",
        "    if score < 0.1:\n",
        "      continue\n",
        "    if answer in blacklist:\n",
        "      continue\n",
        "    if 'bao nhiêu' in sample_question and not any(c.isdigit() for c in answer):\n",
        "      continue\n",
        "    print(score)\n",
        "    print(answer)\n",
        "    print(p_score)\n",
        "    print(p, end='\\n\\n')\n",
        "    if answer not in answers:\n",
        "      answers[answer] = [1, score]\n",
        "    else:\n",
        "      answers[answer][0] += 1\n",
        "      if score > answers[answer][1]:\n",
        "        answers[answer][1] = score"
      ],
      "metadata": {
        "id": "Bc6nRjovHzdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merge similar answers, pick best answer using majority and its confidence"
      ],
      "metadata": {
        "id": "qSVc7y7px8hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keys = list(answers.keys())\n",
        "key_set = set(keys)\n",
        "key_resolve = {}\n",
        "for k in keys:\n",
        "  for sk in key_set:\n",
        "    if k != sk and k in sk:\n",
        "      key_set.remove(k)\n",
        "      key_resolve[k] = sk\n",
        "      break\n",
        "for k, rk in key_resolve.items():\n",
        "  info = answers[k]\n",
        "  answers.pop(k)\n",
        "  answers[rk][0] += info[0]\n",
        "  if info[1] > answers[rk][1]:\n",
        "    answers[rk][1] = info[1]\n",
        "best_answer = None\n",
        "for k, v in answers.items():\n",
        "  print(k, v)\n",
        "  if best_answer is None:\n",
        "    best_answer = k\n",
        "    continue\n",
        "  info = answers[best_answer]\n",
        "  if tuple(info) < tuple(v):\n",
        "    best_answer = k\n",
        "best_answer"
      ],
      "metadata": {
        "id": "Xvmy21GXZxDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Single function for QA pipeline"
      ],
      "metadata": {
        "id": "8l-xOUIsE-dm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_answer(documents, qa_pipeline, question, verbose=False):\n",
        "  question_ngram = NGram(question)\n",
        "  if verbose:\n",
        "    print(f'Question: {question}')\n",
        "\n",
        "  hits = searcher.search(preprocess_text(question), k=100)\n",
        "  document_ranks = []\n",
        "  for hit in hits:\n",
        "    document_ranks.append((hit.score, hit.docid))\n",
        "  if verbose:\n",
        "    for idx, (score, text) in enumerate(document_ranks):\n",
        "      print(idx, score, documents[text][0])\n",
        "    print()\n",
        "\n",
        "  # Stage: Retrieve answers, omit answers with low confidence\n",
        "  passage_ranks = []\n",
        "  for idx, (doc_score, doc) in enumerate(document_ranks):\n",
        "    doc_title, doc_content = documents[doc]\n",
        "    ranks = []\n",
        "    for p in to_passages(doc_content):\n",
        "      pas_ngram_score = question_ngram.score(NGram(p)) + question_ngram.score(NGram(doc_title))\n",
        "      ranks.append((pas_ngram_score, p))\n",
        "    ranks.sort(reverse=True)\n",
        "    passage_ranks += ranks[:5]\n",
        "    if verbose:\n",
        "      for r in ranks[:5]:\n",
        "        print(r[0], question_ngram.score(NGram(doc_title)), doc_title)\n",
        "        print(r[1])\n",
        "  if verbose:\n",
        "    print('\\n')\n",
        "  passage_ranks.sort(reverse=True)\n",
        "\n",
        "  # Stage: Majority vote for finding best answer\n",
        "  answers = {}\n",
        "  blacklist = set(['kiêm', 'Quỳnh'])\n",
        "  for p_score, p in passage_ranks[:50]:\n",
        "    res = qa_pipeline({'question': question, 'context': p})\n",
        "    score = res['score']\n",
        "    answer = res['answer']\n",
        "    answer = re.sub('[,\\\\.\\\\(\\\\);:\\\"]', '', answer)\n",
        "    if answer != '':\n",
        "      if score < 0.1:\n",
        "        continue\n",
        "      if answer in blacklist:\n",
        "        continue\n",
        "      if 'bao nhiêu' in question and not any(c.isdigit() for c in answer):\n",
        "        continue\n",
        "      if verbose:\n",
        "        print(score)\n",
        "        print(answer)\n",
        "        print(p_score)\n",
        "        print(p, end='\\n\\n')\n",
        "      if answer not in answers:\n",
        "        answers[answer] = [1, score]\n",
        "      else:\n",
        "        answers[answer][0] += 1\n",
        "        if score > answers[answer][1]:\n",
        "          answers[answer][1] = score\n",
        "\n",
        "  keys = list(answers.keys())\n",
        "  key_set = set(keys)\n",
        "  key_resolve = {}\n",
        "  for k in keys:\n",
        "    for sk in key_set:\n",
        "      if k != sk and k in sk:\n",
        "        key_set.remove(k)\n",
        "        key_resolve[k] = sk\n",
        "        break\n",
        "  for k, rk in key_resolve.items():\n",
        "    info = answers[k]\n",
        "    answers.pop(k)\n",
        "    answers[rk][0] += info[0]\n",
        "    if info[1] > answers[rk][1]:\n",
        "      answers[rk][1] = info[1]\n",
        "  best_answer = None\n",
        "  for k, v in answers.items():\n",
        "    if verbose:\n",
        "      print(k, v)\n",
        "    if best_answer is None:\n",
        "      best_answer = k\n",
        "      continue\n",
        "    info = answers[best_answer]\n",
        "    if tuple(info) < tuple(v):\n",
        "      best_answer = k\n",
        "  return best_answer"
      ],
      "metadata": {
        "id": "4OtqGz_RqOmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# question = 'Ai hiện là giám đốc điều hành Xiaomi'\n",
        "# question = 'Tổng thống đầu tiên của Mỹ là ai?'\n",
        "# question = 'Trong thần thoại hy lạp vị thần tình yêu có tên là gì?'\n",
        "# question = 'Đạo diễn phim Titanic là ai?'\n",
        "# question = 'Tổng thống Hoa Kỳ thứ 45 là ai'\n",
        "# question = 'Hiện nay ai là tổng bí thư nước Việt Nam'\n",
        "# question = 'Sau khi qua đời, vua Lý Nhân Tông truyền ngôi cho ai'\n",
        "# question = 'Thuyết tương đối đặc biệt và thuyết tương đối tổng quát là ai phát minh ra?'\n",
        "# question = 'Nhà thờ Đức Bà Paris được xây dựng theo lối kiến trúc nào'\n",
        "# question = 'Ngôi chùa đúc hoàn toàn bằng đồng ở Việt Nam'\n",
        "# question = 'Trụ sở chính của Google tên là gì'\n",
        "# question = 'Trương Quang Nghĩa sinh ngày bao nhiêu'\n",
        "question = 'Huyện Mèo Vạc thuộc tỉnh nào của nước ta'"
      ],
      "metadata": {
        "id": "KmwxTEtZPQc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_answer(documents, qa_pipeline, question, verbose=False)"
      ],
      "metadata": {
        "id": "RUTvtd_iFFTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test QA system"
      ],
      "metadata": {
        "id": "RSVn1GAbtDIV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load test data"
      ],
      "metadata": {
        "id": "096YsMsJwKhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tests = []\n",
        "with open('zac2022_testa_sample_submission.json', 'r') as f:\n",
        "  test_data = json.load(f)\n",
        "  for tc in test_data['data']:\n",
        "    tests.append((tc['question'], tc['answer']))"
      ],
      "metadata": {
        "id": "ddynh4iTaU5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run test"
      ],
      "metadata": {
        "id": "SH7JuM20wMkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_answers = []\n",
        "for idx, (q, a) in enumerate(tests):\n",
        "  p = extract_answer(documents, qa_pipeline, q)\n",
        "  test_answers.append(p)\n",
        "  print(f'Test {idx + 1}')\n",
        "  print(f'Question: {q}')\n",
        "  print(f'Answer:   {a}')\n",
        "  print(f'Predict:  {p}')\n",
        "  print()"
      ],
      "metadata": {
        "id": "E_Qu0PfuySAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save test result into file"
      ],
      "metadata": {
        "id": "NTujDSptwXYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_result = []\n",
        "for t, p in zip(tests, test_answers):\n",
        "  test_result.append({'question': t[0], 'truth': t[1], 'answer': p})\n",
        "with open('result.json', 'w', encoding='UTF-8') as f:\n",
        "  json.dump(test_result, f, ensure_ascii=False, indent = 2)"
      ],
      "metadata": {
        "id": "8OGpQiOciX-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ANx9doTo2670"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}